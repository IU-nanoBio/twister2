################################################################################
# Job configuration parameters for submission of twister2 jobs
################################################################################

# twister2 job name
twister2.job.name: "t2-job"

# A Twister2 job can have multiple sets of compute resources
# Four fields are mandatory: cpu, ram, disk and instances
# instances shows the number of compute resources to be started with this specification
# workersPerPod shows the number of workers on each pod in Kubernetes.
#    May be omitted in other clusters. default value is 1.
worker.compute.resources:
- cpu: 0.4  # number of cores for each worker, may be fractional such as 0.5 or 2.4
  ram: 1024 # ram for each worker as Mega bytes
  disk: 1.0 # volatile disk for each worker as Giga bytes
  instances: 2 # number of compute resource instances with this specification
  scalable: false # only one ComputeResource can be scalable in a job
  workersPerPod: 1 # number of workers on each pod in Kubernetes. May be omitted in other clusters.
                   # number of workers using this compute resource: instances * workersPerPod

- cpu: 0.5  # number of cores for each worker, may be fractional such as 0.5 or 2.4
  ram: 1024 # ram for each worker as Mega bytes
  disk: 1.0 # volatile disk for each worker as Giga bytes
  instances: 2 # number of compute resource instances with this specification
  scalable: true # only one ComputeResource can be scalable in a job
  workersPerPod: 1 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

# driver class to run
twister2.job.driver.class: "edu.iu.dsc.tws.examples.internal.rsched.DriverExample"

# worker class to run
twister2.job.worker.class: "edu.iu.dsc.tws.examples.internal.rsched.BasicK8sWorker"
# twister2.job.worker.class: "edu.iu.dsc.tws.examples.internal.comms.BroadcastCommunication"
# twister2.job.worker.class: "edu.iu.dsc.tws.examples.batch.sort.SortJob"
# twister2.job.worker.class: "edu.iu.dsc.tws.examples.internal.BasicNetworkTest"
# twister2.job.worker.class: "edu.iu.dsc.tws.examples.comms.batch.BReduceExample"
# twister2.job.worker.class: "edu.iu.dsc.tws.examples.internal.BasicNetworkTest"
# twister2.job.worker.class: "edu.iu.dsc.tws.examples.batch.comms.batch.BReduceExample"

# by default each worker has one port
# additional ports can be requested for all workers in a job
# please provide the requested port names as a list
twister2.worker.additional.ports: ["port1", "port2", "port3"]

########################################################################
# Kubernetes related settings
########################################################################
# namespace to use in kubernetes
# default value is "default"
# kubernetes.namespace: "default"

########################################################################
# Node locations related settings
########################################################################
# If this parameter is set as true,
# Twister2 will use the below lists for node locations:
#   kubernetes.datacenters.list
#   kubernetes.racks.list
# Otherwise, it will try to get these information by querying Kubernetes Master
# It will use below two labels when querying node locations
# For this to work, submitting client has to have admin privileges
kubernetes.node.locations.from.config: false

# rack label key for Kubernetes nodes in a cluster
# each rack should have a unique label
# all nodes in a rack should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: rack=rack1, rack=rack2, rack=rack3, etc
# no default value is specified
rack.labey.key: rack

# data center label key
# each data center should have a unique label
# all nodes in a data center should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: datacenter=dc1, datacenter=dc1, datacenter=dc1, etc
# no default value is specified
datacenter.labey.key: datacenter

# Data center list with rack names
datacenters.list:
- echo: ['blue-rack', 'green-rack']

# Rack list with node IPs in them
racks.list:
- blue-rack: ['node01.ip', 'node02.ip', 'node03.ip']
- green-rack: ['node11.ip', 'node12.ip', 'node13.ip']

########################################################################
# persistent volume related settings
########################################################################

# persistent volume size per worker in GB as double
# default value is 0.0Gi
# set this value to zero, if you have not persistent disk support
# when this value is zero, twister2 will not try to set up persistent storage for this job
persistent.volume.per.worker: 1.0

# the admin should provide a PersistentVolume object with the following storage class.
# Default storage class name is "twister2".
kubernetes.persistent.storage.class: "twister2-nfs-storage"

# persistent storage access mode. 
# It shows the access mode for workers to access the shared persistent storage.
# if it is "ReadWriteMany", many workers can read and write
# other alternatives: "ReadWriteOnce", "ReadOnlyMany"
# https://kubernetes.io/docs/concepts/storage/persistent-volumes
kubernetes.storage.access.mode: "ReadWriteMany"

# If we need to start the client in remote debugger mode enable this configuration.
# You can change the address port to different one if needed.
# twister2.client.debug: '-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5006'